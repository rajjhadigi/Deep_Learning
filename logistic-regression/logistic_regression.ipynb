import numpy as np
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
#### **Logistic regression from scratch**
class LogisticRegression:
    def __init__(self):
        self.W = None
        self.b = None

    @staticmethod
    def sigmoid(x):
        return 1/(1+np.exp(-x))
    

    def init_weights(self, dim):
        """
            This function will initialize the weights and biases as 0
            Params:
                dim : size of the W vector we want (or number of parameters in this case)
        """
        self.W = np.zeros((dim,1))
        self.b = 0.0

    
    def propagate(self, X,Y):
        """
            Implement the cost function and its gradient for the propagation
            Params:
                X : data of size (features,no_of_examples)
                Y : true "label" (contains 0 or 1) of size(1, no_of_examples)
            Returns:
                cost -- negative log-likelihood cost for logistic regression
                dw -- gradient of the loss with respect to w, thus same shape as w
                db -- gradient of the loss with respect to b, thus same shape as b
        """
        m = X.shape[1]

        # Forward propagation
        A = self.sigmoid(np.dot(self.W.T,X)+self.b)
        cost = -1/m*np.sum(Y*np.log(A) + (1-Y)*np.log(1-A))

        # Backward propagation
        dw = 1/m*np.dot(X,(A-Y).T)
        db = 1/m*np.sum(A-Y)

        cost = np.squeeze(cost) # To make sure cost is a scalar

        grads = {'dw':dw,'db':db}
        return grads, cost



    def optimize(self, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):
        """
            This function optimizes W and b by running a gradient descent algorithm

            Params: 
                X : data of size (features,no_of_examples)
                Y : true "label" (contains 0 or 1) of size(1, no_of_examples)
                num_iterations -- number of iterations of the optimization loop
                learning_rate -- learning rate of the gradient descent update rule
                print_cost -- True to print the loss every 100 steps
        """
        costs = []
        
        for i in range(num_iterations):
            
            grads, cost = self.propagate(X,Y)
            dw = grads['dw']
            db = grads['db']

            self.W = self.W - learning_rate*dw
            self.b = self.b - learning_rate*db

            if i%100 == 0:
                costs.append(cost)
                if print_cost:
                    print("Cost after iteration %i: %f" % (i, cost))

        return costs


    def fit(self, X_train, Y_train, num_iterations=2000, learning_rate=0.5, print_cost=False):
        self.init_weights(X_train.shape[0])
        costs = self.optimize(X_train,Y_train,num_iterations, learning_rate, print_cost)
        return costs

    def predict(self, X):
        m = X.shape[1]
        y_prediction = np.zeros((1,m))
        self.W = self.W.reshape(X.shape[0],1)

        A = self.sigmoid(np.dot(self.W.T,X)+self.b)

        for i in range(A.shape[1]):
            if A[0, i] > 0.5:
                y_prediction[0, i] = 1
            else:
                y_prediction[0, i] = 0

        return y_prediction
# Load the Iris dataset
iris = load_iris()
X = iris.data.T
y = (iris.target != 0).astype(int).reshape(1, -1)
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X.T, y.T, test_size=0.2, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

lr_model = LogisticRegression()
lr_model.fit(X_train.T, y_train.T, print_cost=True)
y_prediction_train = lr_model.predict(X_train.T)
y_prediction_test = lr_model.predict(X_test.T)

print(f"train accuracy: {100 - np.mean(np.abs(y_prediction_train - y_train)) * 100}")
print(f"test accuracy: {100-np.mean(np.abs(y_prediction_test - y_test)) * 100}")
